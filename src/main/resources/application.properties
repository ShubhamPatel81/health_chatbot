spring.application.name=MedianChatob

server.port=9090    

## LangChain4J Configuration for Ollama and DeepSeek R1 Chat Model
#langchain4j.ollama.chat-model.base-url=http://localhost:11434
#langchain4j.ollama.chat-model.model-name=mistra
#langchain4j.ollama.chat-model.strict-tools=true
#langchain4j.ollama.chat-model.temperature=0.5
#langchain4j.ollama.chat-model.timeout=PT60S
#langchain4j.ollama.chat-model.log-requests=false
#langchain4j.ollama.chat-model.log-responses=false
#

logging.level.org.springframework=ERROR
logging.level.dev.langchain4j=DEBUG
logging.level.ai.djl=DEBUG


spring.datasource.username=health
spring.datasource.password=health

spring.datasource.driverClassName=org.h2.Driver
spring.datasource.url=jdbc:h2:mem:health
spring.h2.console.enabled=true
spring.jpa.database-platform=org.hibernate.dialect.H2Dialect
spring.jpa.hibernate.ddl-auto=create
spring.jpa.show-sql=false
